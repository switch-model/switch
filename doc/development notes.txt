Each investment period is a node. The first investment period will be the root node of the scenario tree. It will have one child node for the next investment period and multiple child nodes for dispatch scenarios that could occur within that period. 

In the scenario tree, I need to be able to write stuff like:
set StageVariables[Invest_Stage1] := BuildProjects[Period1,*] RetireExistingPlant[Period1,*] BuildTrans[Period1, *] ...
set StageVariables[Dispatch_Stage1a] := DispatchGen[Scenario1a,*,*]  DispatchTrans[Scenario1a,*,*]  DispatchStorage[Scenario1a,*,*] ...
This duplicates information in the indexes since each timepoint is uniquely associated with a single period, but adding the period indexes will enable much simpler specification of problem stages. I'm currently thinking that stages will progress: Invest1, Dispatch1a, Dispatch1b, Invest2, Dispatch2a, Dispatch2b, ..., Invest-n, Dispatch-n

2/17 PM & 2/18 AM. 
I'm trying to structure and document the code as I go for maximal readability. I still haven't settled on a package/module structure and I will most likely reorganize as I proceed and learn more about python. Right now, I'm aiming for different files to contain python functions that add define major chunks of the switch model. timescales.py is the first of these files. 
I looked into python autodocumentation packages and started using pydoc to get started. I think Sphinx would be better eventually, but that has a bit more set-up and learning to it. Hopefully it won't be too hard to transition text from basic pydoc format (multi-line comments at the beginning of files, functions or classes) to whatever Sphinx prefers. As part of this autodoc effort, I moved most of the inline comments to the multi-line comments. That actually makes the code cleaner and easier to read, in my opinion. 
As I was trying to figure out if pydoc supported any simple markup (it doesn't), I ran into doctest. It's a simple framework that will look for any example code snippets and expected output in the multi-line comments (same ones that pydoc examines). Whatever it finds, it executes and compares literal output to expected output. It's clean and simple, but not terribly scalable. Unittest is the scalable version, but that has a little more learning curve to it, so I'm punting on that for the moment. doctest can integrate with unittest, so whatever simple tests I do with doctest can be re-used later. Anyway, to take advantage of doctest, I merged the contents of the test_timescales() with the SYNOPSIS section. I wrote two more data files that should fail in different ways, but those aren't as appropriate for doctest because part of the error messages include library paths that are specific to my system. Oh well. 

2/28
I rewrote chunks of timescales.py last week, shortening parameter & set names, making them more consistent, moving timepoint weight specifications to timeseries to avoid potential for confusion.. I wrote some notes into a git commit. 

3/2
Pyomo has a reputation for being tricky to debug. A mistake or missing datum can cause funky error messages that don't tell you directly what's going on. Adding validation checks via BuildCheck() can relieve some of this headache, I think. I figured out a way of using python introspection to validate that an arbitrary list of mandatory parameters has been specified. It seems overkill for base financial info where I wrote it, because the list only has two members. Still, it's a useful bit of code to retain, so I'm pasting it here. 
	def validate_minimum_financial_data_rule (mod):
		mandatory_params = set(['base_financial_year', 'interest_rate'])
		for active_param in mod.active_components(Param):
			if ( active_param in mandatory_params ):
				obj = getattr(mod, active_param)
				if ( obj() is None ):
					print "Mandatory parameter " + active_param + " is missing!"
					return 0
		return 1
	switch.minimum_data_requirements = BuildCheck(rule=validate_minimum_financial_data_rule)

3/4
I tried using .dat file load commands to load .tab files because I thought this might be needed for some PySP scenario tree stuff. 
See https://software.sandia.gov/downloads/pub/pyomo/PyomoOnlineDocs.html#_data_command_files
I could only get it to work when the .tab files were in the current working directory. Specifying a path such as test_dat/dispatch_scenarios.tab instead of dispatch_scenarios.tab generated an error, complaining that / is an illegal character. Wrapping the path in quotes generated an even more obscure error message during model instantiation. The documentation didn't help at all with this. I read the code that parses these commands (pyomo/core/data/parse_datacmds.py, ampl.py, proess_data.py), but couldn't find a way of specifying a directory name.
Anyway, the following commands work when the .tab files are in the working directory. 
	load periods.tab : INVEST_PERIODS = [INVESTMENT_PERIOD] period_start period_end;
	load dispatch_scenarios.tab : DISPATCH_SCENARIOS = [DISPATCH_SCENARIO] disp_scen_period=period disp_scen_dbid=dbid;
	load timeseries.tab : TIMESERIES=[TIMESERIES] ts_disp_scen ts_duration_of_tp ts_num_tps ts_scale_to_period;
	load timepoints.tab : TIMEPOINTS=[timepoint_id] tp_label=timepoint_label tp_ts=timeseries;

3/6
Today I pushed my boundaries of using DataPortal: loading parameters with multi-dimensional indexing. I encountered numerous problems that ultimately were rooted in the data file that I thought I was using really being in a different directory. Sublime doesn't update itself like BBEdit if an open file is moved to a new directory in the background. 
I learned that DataPortal.load() is sparsely documented and doesn't appear at all in the Pyomo book, even though the website documentation for DataPortal references the book. https://software.sandia.gov/downloads/pub/pyomo/PyomoOnlineDocs.html#_dataportal_objects
DataPortal seems to have suplanted the ModelData class. The load function in the DataPortal object has a bunch of TODO's in place of desscriptive text in its inline documentation. 
I learned that if you want to select just one column from a text .tab file with load(), you need to wrap the column name in a tuple (not a list), or else the argument processing code in TableData.py will break it down character-by-character. So this works
    switch_data.load(
        filename=os.path.join(inputs_directory, 'load_zones.tab'),
        select=['balancing_area'],
        param=mod.foo)
But select='balancing_area' and select=('balancing_area') both generate an error:
"ValueError: 'b' is not in list", meaning the list of header columns in the .tab file. 
You cannot build a set from a single column of a multi-column file. If you read a file into a set, the parser will ignore the select clause, will assume the set is multi-dimensional, cast the entire row as a tuple, try to append that tuple to the set, and generate an error message when the set complains about a dimensionality mis-match. 

To print debugging messages in my code or the Pyomo code, this code injection works well:
	import logging
	logging.warning('obj value is')
	logging.warning(obj)

To deal with balancing areas being optional, I ended up initializing them to the unique values in the balancing_area column of load_zones.tab. Then I read in from the balancing_areas.tab file if it exists. If I set index=mod.BALANCING_AREAS, then this file overrides the initial value. Interesting. One upshot is that I still need to check that balancing area names match between load zone data and balancing area data if I specify the index. I can also leave the index clause out completely if I specify the index as the first column in the select clause like so:
    switch_data.load(
        filename=balancing_area_path,
        select=(
            'BALANCING_AREAS', 'quickstart_res_load_frac',
            'quickstart_res_wind_frac', 'quickstart_res_solar_frac',
            'spinning_res_load_frac', 'spinning_res_wind_frac',
            'spinning_res_solar_frac'),
        #index=mod.BALANCING_AREAS,
        param=(mod.quickstart_res_load_frac, mod.quickstart_res_wind_frac,
               mod.quickstart_res_solar_frac, mod.spinning_res_load_frac,
               mod.spinning_res_wind_frac, mod.spinning_res_solar_frac))
This formulation actually generates a reasonable error message if there is a balancing area in this file that doesn't match the pre-existing set. For example, I have NorthCentral and South defined in load_zones.tab. If I specify South_typo in balancing_areas.tab, I get the error:
    RuntimeError: Failed to set value for param=quickstart_res_load_frac, index=South_typo, value=0.04.
    	source error message="Error setting parameter value: Index 'South_typo' is not valid for array Param 'quickstart_res_load_frac'"

3/18/2015
The implementation of the fuels module went slower than I liked. I did manage to merge all of the custom hacks into a single uniform implementation that relied on regional fuel markets with supply curves with optional load-zone level flat cost adjustments. To support NG price elasticity, biogas limits and biomass supply curves we need some code for supply curves and regional price adjustments, and the code complexity doesn't increase if that code supports that functionality for all fuels. Still, this module ended up a bit messier than others.

I wanted to support an optional input file that specified simple costs for fuels per load zone and period without upper limits on consumption, and I learned how to manipulate Pyomo's DataPortal() object to accomplish this. Basically, it stores everything it reads in in a nested dictionary. Accessing one of the data components (sets or parameters at least) for reads or writes is just a matter of using the data() function. A read example from the switch_data object: switch_data.data(name='FUELS'). In theory, I could also access it with switch_data.data()['FUELS'], but this returns a dictionary of FUELS indexed by namespaces, with a default namespace of 'None':
>>> switch_data.data()['FUELS']
{None: ['Coal', 'ResidualFuelOil', 'DistillateFuelOil', 'NaturalGas', 'Uranium', 'BioSolid']}
This namespace indexing is not consistent between sets and parameters, as you can see with the example from the parameter f_co2_intensity:
>>> switch_data.data()['f_co2_intensity']
{'ResidualFuelOil': 0.0788, 'Uranium': 0, 'DistillateFuelOil': 0.07315, 'Coal': 0.09552, 'NaturalGas': 0.05306, 'BioSolid': 0.09435}
I can get uniform behavior of avoiding namespace indexing if I use switch_data.data(name='FUELS') and switch_data.data(name='f_co2_intensity'), so that is my preferred usage:
>>> switch_data.data(name='FUELS')
['Coal', 'ResidualFuelOil', 'DistillateFuelOil', 'NaturalGas', 'Uranium', 'BioSolid']
>>> switch_data.data(name='f_co2_intensity')
{'ResidualFuelOil': 0.0788, 'Uranium': 0, 'DistillateFuelOil': 0.07315, 'Coal': 0.09552, 'NaturalGas': 0.05306, 'BioSolid': 0.09435}
You can directly manipulate this objects, adding new elements or overwriting components, and that data will be used when a model instance is instantiated. 
>>> switch_data.data(name='FUELS').append('SolidWaste')
>>> switch_data.data(name='f_co2_intensity')['SolidWaste'] = 0.095
>>> switch_data.data(name='FUELS')
['Coal', 'ResidualFuelOil', 'DistillateFuelOil', 'NaturalGas', 'Uranium', 'BioSolid', 'SolidWaste']
>>> switch_data.data(name='f_co2_intensity')
{'ResidualFuelOil': 0.0788, 'Uranium': 0, 'DistillateFuelOil': 0.07315, 'Coal': 0.09552, 'SolidWaste': 0.095, 'NaturalGas': 0.05306, 'BioSolid': 0.09435}

3/19/2015
I'm thinking about how to implement hybrid facilities to reduce excessive code and related complexities. By hybrid facility, I mean a generator uses multiple energy sources or perhaps separate generating units are linked. The key use cases are:
• Pumped Hydro which includes a storage component and a water-flowing-downhill component
• Compressed Air Energy Storage which includes a storage component and a gas turbine component
Other use cases could be:
• Any combined cycle plant if the gas and steam generators are modeled separately
• Solar thermal with fossil or biomass backup boilers
• Cofiring Coal & biomass
• Oil generators that can run on multiple fuels or need to start on a higher grade fuel before switching to a lower grade fuel

The last three use cases could be modeled as plants that can run on multiple energy sources rather than separate generators with linked investment and operations. 

The status quo for hybrid plants that include a storage plant was to add storage decision variables and attributes, write a number of extra terms and constraints to link their storage and non-storage components, and export their hourly dispatch decisions in two or three rows, for the non-storage component, the store/release of REC eligible energy, and the store/release of non-renewable energy. 

The physical structure of pumped hydro & CAES has a single generator that produced all energy and a pump that stores energy. We tracked ReleaseEnergy separately from DispatchEnergy so that we could track Renewable Energy Credits into and out of storage for modeling renewable portfolio standards. At the time we wrote this, policy hadn't caught up with the idea of tracking renewable energy through storage. As of 2013, some California RPS documentation explicitly addresses this on pages 43-46 and 64-65 of 
http://www.energy.ca.gov/2013publications/CEC-300-2013-005/CEC-300-2013-005-ED7-SD.pdf
This say that storage projects can only count as renewable energy if they are directly connected to a renewable facility and generally owned by the same entity. Any energy that comes out of stand-alone storage projects is not eligible for RECs, even if some renewable energy was stored in it. So, if you are going to store renewable energy in a generic grid storage device, then you should unbundle the REC component before storage to maintain full economic value.

I guess the other reason to track released energy is to ensure energy balancing in the storage vessel. We could tie this released energy to the dispatch variable to reflect that it is consumed by the generator to produce electricity, but it could be difficult to parse out how much of the dispatch is coming from storage vs the other energy source, at least for pumped hydro.
